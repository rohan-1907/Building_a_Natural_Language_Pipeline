{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O7JdmzhVRpAE"
   },
   "outputs": [],
   "source": [
    "# NLTK Stands for Natural Language Toolkit\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Import OS Library\n",
    "import os\n",
    "\n",
    "# Import string library\n",
    "import string\n",
    "\n",
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Import numpy\n",
    "import numpy as np\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Import math\n",
    "import math\n",
    "\n",
    "# Import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import (\n",
    "    CountVectorizer,\n",
    "    TfidfVectorizer\n",
    ")\n",
    "\n",
    "sw = stopwords.words('English')\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4GbhjxdJRpAM"
   },
   "source": [
    "## Preparing Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d-0WSfJLRpAN"
   },
   "source": [
    "### Loading the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vu34zcaFRpAS"
   },
   "source": [
    "loading disaster tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vFW0qnK0RpAU"
   },
   "outputs": [],
   "source": [
    "tweets_data = pd.read_csv('data/tweets_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bbLMUAfVRpAV"
   },
   "source": [
    "Let's see how many examples we have for each class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6RlIdJNeRpAY",
    "outputId": "1b65bd4e-8197-458e-81d9-ae228d483208",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4342\n",
       "1    3271\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_data.target.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Az7et7aZRpAb"
   },
   "source": [
    "***\n",
    "Now, I'll define a function that will contain several choices regarding the NLP\n",
    "Pipeline.\n",
    "<br>\n",
    "<br>\n",
    "Ideally, its usually done by building several functions in this pipeline, probably as a class, but I had done this with a single big function that can be controled using parameters to make it easier.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lMfYWboyRpAf"
   },
   "source": [
    "### Building the Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jh8lUWTkRpAi"
   },
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    data,\n",
    "    stop_words,\n",
    "    stem,\n",
    "    remove_punct,\n",
    "    vectorizer,\n",
    "    min_df,\n",
    "    ngram\n",
    "):\n",
    "    '''\n",
    "    Builds pipeline, trains a model and evaluates \n",
    "    on training and test set.\n",
    "    \n",
    "    Each step is fired according to values coming from the\n",
    "    parameters.\n",
    "    '''\n",
    "    \n",
    "    # Tokenizing the Text\n",
    "    tokenized_tweets = (\n",
    "        data[\"text\"].apply(word_tokenize)\n",
    "    )\n",
    "    \n",
    "    # Check if Stop Words should be removed\n",
    "    if stop_words:\n",
    "        tokenized_tweets = (\n",
    "            tokenized_tweets.apply(lambda x: [word.lower() for word in x if word not in sw])\n",
    "        )\n",
    "        \n",
    "    # Check if we should stem words\n",
    "    if stem:\n",
    "        tokenized_tweets = (\n",
    "            tokenized_tweets.apply(lambda x: [stemmer.stem(word) for word in x])\n",
    "        )\n",
    "\n",
    "    tokenized_tweets_sentence = (\n",
    "        tokenized_tweets.apply(lambda x: ' '.join(x))\n",
    "    )\n",
    "    \n",
    "    # Check if we should remove punctuation\n",
    "    if remove_punct:\n",
    "\n",
    "        tokenized_tweets_sentence = tokenized_tweets_sentence.apply(lambda x: \n",
    "            x.translate(\n",
    "                str.maketrans('', '', string.punctuation)\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Checks what vectorizer we should apply\n",
    "    if vectorizer == 'count':\n",
    "        cv = CountVectorizer(min_df = min_df, ngram_range = ngram)\n",
    "        text_data = cv.fit_transform(tokenized_tweets_sentence).todense()\n",
    "        \n",
    "    if vectorizer == 'tfidf':\n",
    "        cv = TfidfVectorizer(min_df = min_df, ngram_range = ngram)\n",
    "        text_data = cv.fit_transform(tokenized_tweets_sentence).todense()  \n",
    "        \n",
    "    if vectorizer == 'binary':\n",
    "        cv = CountVectorizer(binary=True, min_df = min_df, ngram_range = ngram)\n",
    "        text_data = cv.fit_transform(tokenized_tweets_sentence).todense()\n",
    "\n",
    "    # Builds features\n",
    "    X = pd.DataFrame(\n",
    "        text_data,\n",
    "        columns=cv.get_feature_names()\n",
    "    )\n",
    "    # Builds target\n",
    "    y = data.target\n",
    "\n",
    "    # Divide into train and test\n",
    "    X_train, X_test, y_train, y_test = (\n",
    "        train_test_split(X, y, test_size = 0.2, random_state=1234)\n",
    "    )\n",
    "    # Build Model\n",
    "    lm = LogisticRegression(random_state=1234)\n",
    "    lm.fit(X_train, y_train)\n",
    "    y_train_pred = lm.predict(X_train)\n",
    "    y_test_pred = lm.predict(X_test)\n",
    "\n",
    "    # Check the accuracy score for train and test\n",
    "    train_score = accuracy_score(y_train, y_train_pred)\n",
    "    test_score = accuracy_score(y_test, y_test_pred)\n",
    "    \n",
    "    return train_score, test_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ypnKkSK5RpAn"
   },
   "outputs": [],
   "source": [
    "# Build experiments with different parameters\n",
    "experiments = {\n",
    "    '1': {'stop_words': True, 'stem': True, 'remove_punct': True, 'vectorizer': 'count', 'min_df': 0.02, 'ngram' : (1,1)},\n",
    "    '2': {'stop_words': True, 'stem': True, 'remove_punct': False, 'vectorizer': 'count', 'min_df': 0.02, 'ngram' : (1,1)},\n",
    "    '3': {'stop_words': True, 'stem': True, 'remove_punct': False, 'vectorizer': 'count', 'min_df': 0.005, 'ngram' : (1,1)},\n",
    "    '4': {'stop_words': True, 'stem': True, 'remove_punct': False, 'vectorizer': 'count', 'min_df': 2, 'ngram' : (1,1)},\n",
    "    '5': {'stop_words': True, 'stem': True, 'remove_punct': False, 'vectorizer': 'tfidf', 'min_df': 0.005, 'ngram' : (1,1)},\n",
    "    '6': {'stop_words': True, 'stem': True, 'remove_punct': False, 'vectorizer': 'tfidf', 'min_df': 0, 'ngram' : (1,1)},\n",
    "    '7': {'stop_words': True, 'stem': False, 'remove_punct': False, 'vectorizer': 'tfidf', 'min_df': 0, 'ngram' : (1,1)},\n",
    "    '8': {'stop_words': False, 'stem': False, 'remove_punct': False, 'vectorizer': 'tfidf', 'min_df': 0, 'ngram' : (1,1)},\n",
    "    '9': {'stop_words': False, 'stem': True, 'remove_punct': False, 'vectorizer': 'tfidf', 'min_df': 0, 'ngram' : (1,1)},\n",
    "    '10': {'stop_words': False, 'stem': True, 'remove_punct': False, 'vectorizer': 'binary', 'min_df': 0, 'ngram' : (1,1)}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AWFAgwytRpAp",
    "outputId": "eea3601c-e947-4f7f-e9e8-063128995a65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for experiment 1 the results are (0.6336617405582923, 0.6250820748522653)\n",
      "for experiment 2 the results are (0.6366174055829228, 0.6172028890347997)\n",
      "for experiment 3 the results are (0.8060755336617406, 0.7846355876559422)\n",
      "for experiment 4 the results are (0.9313628899835796, 0.8030203545633617)\n",
      "for experiment 5 the results are (0.7986863711001642, 0.7846355876559422)\n",
      "for experiment 6 the results are (0.8863711001642036, 0.8082731451083388)\n",
      "for experiment 7 the results are (0.8893267651888341, 0.8030203545633617)\n",
      "for experiment 8 the results are (0.8906403940886699, 0.7984241628365069)\n",
      "for experiment 9 the results are (0.8855500821018062, 0.8082731451083388)\n",
      "for experiment 10 the results are (0.9650246305418719, 0.8056467498358503)\n"
     ]
    }
   ],
   "source": [
    "# Run experiments for multiple parameters\n",
    "for exp in experiments.items():\n",
    "    parameters = exp[1]\n",
    "    print('for experiment '+exp[0]+' the results are '+str(train_model(\n",
    "        tweets_data,\n",
    "        parameters['stop_words'],\n",
    "        parameters['stem'],\n",
    "        parameters['remove_punct'],\n",
    "        parameters['vectorizer'],\n",
    "        parameters['min_df'],\n",
    "        parameters['ngram']\n",
    "    )))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ESt2dllARpAr"
   },
   "source": [
    "The experiment with best score on the training set is **experiment number 10.**\n",
    "<br>\n",
    "The experiment with best balance between training and test accuracy is **experiment number 6.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "02 - Building a Natural Language Pipeline.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
